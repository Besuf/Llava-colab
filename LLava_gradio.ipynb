{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpaC8vuVTsEp"
   },
   "outputs": [],
   "source": [
    "model_path = \"liuhaotian/llava-v1.6-vicuna-7b\" # @param {\"type\":\"string\",\"placeholder\":\"\\\"liuhaotian/llava-v1.6-vicuna-7b\\\"\"}\n",
    "%cd /content\n",
    "\n",
    "import os, sys\n",
    "\n",
    "# Clone or update LLaVA repo\n",
    "if not os.path.exists(\"/content/LLaVA\"):\n",
    "    !git clone https://github.com/xjdeng/LLaVA\n",
    "else:\n",
    "    !git -C /content/LLaVA pull --ff-only\n",
    "\n",
    "%cd /content/LLaVA\n",
    "\n",
    "# Clone or update Llava-colab helper repo\n",
    "if not os.path.exists(\"/content/LLaVA/Llava-colab\"):\n",
    "    !git clone https://github.com/Besuf/Llava-colab\n",
    "else:\n",
    "    !git -C /content/LLaVA/Llava-colab pull --ff-only\n",
    "\n",
    "# Install a CUDA-compatible PyTorch for Colab\n",
    "try:\n",
    "    import torch\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "except Exception:\n",
    "    has_cuda = False\n",
    "\n",
    "if has_cuda:\n",
    "    # CUDA 12.1 wheels for recent Colab runtimes\n",
    "    !pip install -q --upgrade \"torch==2.4.0\" \"torchvision==0.19.0\" \"torchaudio==2.4.0\" --index-url https://download.pytorch.org/whl/cu121\n",
    "else:\n",
    "    !pip install -q --upgrade \"torch==2.4.0\" \"torchvision==0.19.0\" \"torchaudio==2.4.0\"\n",
    "\n",
    "# Core Python deps (pin versions with prebuilt wheels to avoid build failures)\n",
    "!pip install -q --upgrade \"transformers==4.36.2\" \"tokenizers==0.15.2\" \"accelerate>=0.23.0\" \"bitsandbytes>=0.43.1\" \"gradio>=3.45.0\" \"einops\" \"sentencepiece\" \"pillow\"\n",
    "\n",
    "# Install LLaVA package but skip its strict dependency pins (avoids torch==2.0.1)\n",
    "!pip install -q -e . --no-deps\n",
    "\n",
    "# Patch LLaVA to avoid duplicate 'llava' AutoConfig registration conflicts with Transformers\n",
    "# This makes the registration idempotent if Transformers already provides a 'llava' entry\n",
    "!python - <<'PY'\n",
    "import os, re\n",
    "p = 'llava/model/language_model/llava_llama.py'\n",
    "if os.path.exists(p):\n",
    "    s = open(p, 'r', encoding='utf-8').read()\n",
    "    new = re.sub(r'AutoConfig\\.register\\(\"llava\",\\s*LlavaConfig\\)',\n",
    "                 'AutoConfig.register(\"llava\", LlavaConfig, exist_ok=True)', s)\n",
    "    if new != s:\n",
    "        open(p, 'w', encoding='utf-8').write(new)\n",
    "        print('Patched: AutoConfig.register(\"llava\", LlavaConfig, exist_ok=True)')\n",
    "    else:\n",
    "        print('No patch needed for AutoConfig.register; pattern not found or already patched')\n",
    "else:\n",
    "    print('Patch skipped: file not found', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1fyCLtuU7C0"
   },
   "outputs": [],
   "source": [
    "%cd Llava-colab\n",
    "!git pull\n",
    "%cd ..\n",
    "!cp Llava-colab/backend.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIg3LjMqUC8w"
   },
   "outputs": [],
   "source": [
    "!python backend.py $model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VC8JQNrrVaN0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMRN8xdg7ADcnS8x5ABL6g6",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
